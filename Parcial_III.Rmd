---
title: "Parcial III"
author: "Alimi Garmendia 14-10392"
date: "3/13/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Tarea : Ejercicio 3
#### Exámen: Ejercicios 1,2,4,6,7


## 1. Análisis de Componentes Principales: Realice un análisis de componentes principales sobre los porcentajes de votación por partido. Este debe incluir con cuantos componentes se debe trabajar luego del análisis y que expresan cada uno de esos componentes.

```{r include = FALSE}

library(dplyr)
library(ggplot2)
library(factoextra)
library(NbClust)
library(gridExtra)
library(dplyr)
library(CCA)
library(tidyverse)
library(psych)
library(corrplot)
library(corrr)
library(stats)
# Tidydata
data <- read.csv("Data/Raw/Elecciones.csv", header = T, sep = "\t")
new_data <- cbind(data[, 1:2], data[,5:6],data[,9:10],data[,15:16],data[,21:24])
write.csv(new_data,"Data/Tidy/Elecciones_Parcial.csv")

data <- read.csv("Data/Tidy/Elecciones_Parcial.csv",header = T,)[,2:13]
options(digits = 4)
```
### Comenzamos análizando los datos para el pártido demócrata.
```{r }
demo <- data %>%
  select(Estado,Cod,DEM12,DEM04,DEM92,DEM80,Tend,REG)

pca.demo <- princomp(demo[,3:6],cor = T)
summary(pca.demo)

```
Como podemos ver con sólo dos componentes podemos representar aproximadamente 96% de la varianza de los datos
Podemos verlo de manera gráfica con las siguientes figuras
```{r echo = FALSE,fig.align='center'}
prop_varianza <- pca.demo$sdev^2 / sum(pca.demo$sdev^2)
ggplot(data = data.frame(prop_varianza, pca.demo = 1:4),
       aes(x = pca.demo, y = prop_varianza)) +
  geom_col(width = 0.3)+
  theme_bw()+labs(x = "Componente Princial",y = "Prop. de varianza explicada")

prop_varianza_acum <- cumsum(prop_varianza)
p <- ggplot(data = data.frame(prop_varianza, pc = 1:4),
            aes(x = pc, y = prop_varianza_acum, group =1))
p + geom_point()+geom_line()+labs(x = "Componente principal", 
                                  y = "Prop. varianza explicada acumulada")+
  geom_label(aes(label = round(prop_varianza_acum,2))) 

```
Como era de esperarse, la primera componente es la que describe la mayor parte de la varianza. De igual forma podemos ver que con dos componentes podemos describir una gran porción de la varianza. Si revisamos el criterio de Keiser.
```{r}
eigen(cor(demo[,3:6]))$values
mean(eigen(cor(demo[,3:6]))$values)
```
A pesar de ser menor que la media, la segunda componente es tomada pues añade un importante 16% a la varianza acumulada, sin embargo las demás componentes pueden ser obviadas.

Podemos visualizar las dos componentes de la forma:
```{r echo = FALSE}
plot(pca.demo$scores[,1],pca.demo$scores[,2],pch=21,bg = demo$REG)
legend(3,-1,legend = levels(demo$REG), col = c("red","blue","green","black"),lty = 1:2,cex = 0.7)

par(mfrow=c(2,1), bg="azure")
barplot(loadings(pca.demo)[,1],col="blue2",sub="Primera componente")
barplot(loadings(pca.demo)[,2],col="orange",sub="Segunda componente")
```


  Como hemos visto, con dos componentes es suficiente para representar nuestros datos
  
### Ahora realizamos el análisis para el pártido repúblicano

```{r}
gop <- data %>%
  select(Estado,Cod,GOP12,GOP04,GOP92,GOP80,Tend,REG)

pca.gop <- princomp(gop[,3:6],cor = T)
summary(pca.gop)

```
Como podemos ver con sólo dos componentes podemos representar aproximadamente 93% de la varianza de los datos
Podemos verlo de manera gráfica con las siguientes figuras
```{r echo = FALSE}
prop_varianza <- pca.gop$sdev^2 / sum(pca.gop$sdev^2)
ggplot(data = data.frame(prop_varianza, pca.gop = 1:4),
       aes(x = pca.gop, y = prop_varianza)) +
  geom_col(width = 0.3)+
  theme_bw()+labs(x = "Componente Princial",y = "Prop. de varianza explicada")

prop_varianza_acum <- cumsum(prop_varianza)
p <- ggplot(data = data.frame(prop_varianza, pc = 1:4),
            aes(x = pc, y = prop_varianza_acum, group =1))
p + geom_point()+geom_line()+labs(x = "Componente principal", 
                                  y = "Prop. varianza explicada acumulada")+
  geom_label(aes(label = round(prop_varianza_acum,2))) 


```
Como era de esperarse, la primera componente es la que describe la mayor parte de la varianza. De igual forma podemos ver que con dos componentes podemos describir una gran porción de la varianza. Si revisamos el criterio de Keiser.
```{r}
eigen(cor(gop[,3:6]))$values
mean(eigen(cor(gop[,3:6]))$values)
```
A pesar de ser menor que la media, la segunda componente es tomada pues añade un aceptable 8% a la varianza acumulada, sin embargo las demás componentes pueden ser obviadas.

Podemos visualizar las dos componentes de la forma:
```{r echo = FALSE}
plot(pca.gop$scores[,1],pca.gop$scores[,2],pch=21,bg = gop$REG)
legend(-8,1,legend = levels(gop$REG), col = c("red","blue","green","black"),lty = 1:2,cex = 0.7)


par(mfrow=c(2,1), bg="azure")
barplot(loadings(pca.gop)[,1],col="blue2",sub="Primera componente")
barplot(loadings(pca.gop)[,2],col="orange",sub="Segunda componente")

```

## 2. Análisis de Conglomerados: Realice un análisis de conglomerados utilizando dos métodos jerárquicos + el método de k-medias para agrupar los 50 estados + el Distrito de Columbia según los porcentajes de votación. Exprese el número de conglomerados que escogería, y que entidades estarían en cada uno de esos conglomerados.

Para el cálculo de las distancias en ambos métodos utilizaremos la distancia de Manhattan.Comenzaremos usando el método de K-means para varios K's

```{r echo=FALSE}
data
numeric.values <- data[,3:10]
rownames(numeric.values) = data$Cod
america.dist <- get_dist(numeric.values, method = "manhattan")
fviz_dist(america.dist, lab_size = 8, gradient = list(low = "white",mid=NULL, high ="red"))
algoritmo="kmeans" #k-medias
p <- c()
for (i in c(2,3,4,5)){
  set.seed(1)
  cuantosClusters=i
  solucionKmeans1 <- eclust(numeric.values,
                            FUNcluster = algoritmo,
                            k = cuantosClusters, #como lo hicimos previamente
                            graph =F, hc_metric = "manhattan")
  p <- cbind(p,solucionKmeans1)
  print(fviz_silhouette(solucionKmeans1) + ggtitle(sprintf("k = %s", i)))
}
p1 <- fviz_cluster(as.list(p[,1]), data = numeric.values) + ggtitle("k = 2")
p2 <- fviz_cluster(p[,2], data = numeric.values) + ggtitle("k = 3")
p3 <- fviz_cluster(p[,3], data = numeric.values) + ggtitle("k = 4")
p4 <- fviz_cluster(p[,4], data = numeric.values) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Ahora nos dedicaremos a averiguar ¿Cuántos Clusters deberíamos usar?
```{r echo = FALSE}
 ncong = NbClust(data=numeric.values, distance = NULL, diss = america.dist, min.nc=2,
                max.nc=10, method = "kmeans", index = "alllong")
fviz_nbclust(ncong, verbose =T) 

fviz_nbclust(numeric.values, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

fviz_nbclust(numeric.values, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

set.seed(123)
fviz_nbclust(numeric.values, kmeans, nstart = 2,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
```

Como podemos ver, algunos criterio sugieren que un cluster es suficiente, lo cual anularía el propósito de clusterización, sin embargo la mayoría de los criterios sugieren que K = 3 es el número óptimo de clusterización.

```{r echo=FALSE}
set.seed(100)
solucionKmeans1 <- eclust(numeric.values,
                            FUNcluster = algoritmo,
                            k = 3, #como lo hicimos previamente
                            graph =T, hc_metric = "manhattan",seed = 100)
```

Realizando ahora, por el método jerárquico

```{r echo = FALSE}
rownames(numeric.values) = data$Estado
res.hc2 <- eclust(numeric.values, "hclust", k = 3, hc_metric = "manhattan", seed = 100) # compute hclust
fviz_dend(res.hc2,rect = TRUE)
```


En el dendograma podemos ver como podrían dividirse los estados. 
A diferencia del método de K-Medias, elmétodo jerárquico úbica al DC en un grupo separado de los demás estados, esto posiblemente debido a la prominente diferencia e inclinación hacia el partido demócrata de
éste distrito, cosa que se puede apreciar mejor en el HeatMap anterior.

Por lo que el modelo propuesto por el método jerárquico podría ser el mejor para usar.



## 3. Análisis de Correlación Canónica: Realice un análisis de correlación canónica de las variables que se refieren a los porcentajes de votación por el partido repúblicano contra las variables que se refieren a los porcentajes de votación por el partido demócrata. Escriba cuales son los modelos lineales que obtendría, y cuales son las correlaciones entre las nuevas variables y las variables originales.

Comenzamos separándo los datos por partido
```{r}
demo <- data %>%
  select(DEM12,DEM04,DEM92,DEM80)
gop <- data %>%
  select(GOP12,GOP04,GOP92,GOP80)
```
Luego cálculamos la correlación entre las nuevas variables y la correlación cruzada.

```{r}
correl <- matcor(demo,gop)
img.matcor(correl, type = 2)
```
Como era de esperarse, la correlación cruzada es principalmente negativa, el que crezca el porcentaje del GOP disminuye el de los demócratas.

```{r}
cc1 <- cc(demo,gop)
barplot(cc1$cor, main = "Canonical correlations for 'cancor()'", col = "gray")
cc1$xcoef
plt.cc(cc1, var.label = TRUE, ind.names = data[,2])
```
  
  Nuevamente podemos ver que el DC parece ser atípico entre los demás estados y como las variables que representan los resultados de los demócratas y repúblicanos , parecen estar correlacionadas entre sí mismas mas que entre ambos partidos. Sin embargo vemos como ningún año en específico es más representativo que los demás para la variabilidad de los datos.

## 4. Análisis de Correspondencias:

Comenzamos por añadir una forma de dividir los datos según lo pedido por el enunciado.  

* A = > 10% a favor de repúblicanos.

* B Entre 2% y 10% a favor de repúblicanos.

* C < 2% de diferencia entre los dos partidos.

* D Entre 2% y 10% a favor de demócratas.

* E > 10% a favor de demócratas.

```{r include=FALSE}
votes = read.csv("Data/Tidy/Elecciones_Parcial.csv", header = TRUE)[,2:13]
attach(votes)
addyear12 <- function(demi,repi,dataa){
  testi = repi - demi
  dataa <- dataa %>%
    mutate(Y12 = ifelse( testi >= 10 , "A",ifelse( testi < 10 & testi >2, "B",
                                                      (ifelse( testi < 2 & testi > -2, "C",
                                                               (ifelse(testi < -2 & testi > -10,"D",
                                                                       (ifelse(testi < -10, "E",0))
                                                               )))))))
  return(dataa)

}
addyear04 <- function(demi,repi,dataa){
  testi = repi - demi
  dataa <- dataa %>%
    mutate(Y04 = ifelse( testi >= 10 , "A",ifelse( testi < 10 & testi >2, "B",
                                                   (ifelse( testi < 2 & testi > -2, "C",
                                                            (ifelse(testi < -2 & testi > -10,"D",
                                                                    (ifelse(testi < -10, "E",0))
                                                            )))))))
  return(dataa)
  
}
addyear92 <- function(demi,repi,dataa){
  testi = repi - demi
  dataa <- dataa %>%
    mutate(Y92 = ifelse( testi >= 10 , "A",ifelse( testi < 10 & testi >2, "B",
                                                   (ifelse( testi < 2 & testi > -2, "C",
                                                            (ifelse(testi < -2 & testi > -10,"D",
                                                                    (ifelse(testi < -10, "E",0))
                                                            )))))))
  return(dataa)
  
}
addyear80 <- function(demi,repi,dataa){
  testi = repi - demi
  dataa <- dataa %>%
    mutate(Y80 = ifelse( testi >= 10 , "A",ifelse( testi < 10 & testi >2, "B",
                                                   (ifelse( testi < 2 & testi > -2, "C",
                                                            (ifelse(testi < -2 & testi > -10,"D",
                                                                    (ifelse(testi < -10, "E",0))
                                                            )))))))
  return(dataa)
  
}
datab = addyear80(GOP80,DEM80,votes)
datab = addyear92(GOP92,DEM92,datab)
datab = addyear04(GOP04,DEM04,datab)
datab = addyear12(GOP12,DEM12,datab)
```
Comenzamos añadiendo las nuevas variables a nuestro data set por año, resultando:
```{r echo = FALSE}
head(datab)
```
Creamos la tabla de contingencia contando las ocurrencias de A,B,c,D y E por región 

```{r echo=FALSE}
datac <- datab %>%
  select(REG,Y80,Y92,Y04,Y12)

tablea <- as.table(as.matrix(xtabs(~REG+Y80,datac))+as.matrix(xtabs(~REG+Y92,datac))+as.matrix(xtabs(~REG+Y04,datac))+as.matrix(xtabs(~REG+Y12,datac)))

tablea
```
Así creamos realizamos nuestro análisis de correspondencias

```{r}
library('ca')
rel.ca <- ca(tablea)
plot.ca(rel.ca,mass=c(T,T),col = c('red','blue'))
```

Como podemos ver los estados de la región _South_ tienen una tendencia a favorecer, por un bajo margen, a los demócratas. Por otro lado las zonas de _Midwest_ y _WEST_ suelen ser de mayoría democrática. La región _Northest_ tiende a ser más favorecedora del GOP.

## 6. Análisis Factorial:Realice un análisis factorial sobre los datos. Consiga el número de factores que serían suficientes para explicar los datos. Escriba los modelos de cada uno de los factores, y los puntajes factoriales que se obtendrían.

Comenzamos calculando la matriz de covarianzas de los datos
```{r include=FALSE}
usa <- data %>%
  select(DEM12,GOP12,DEM04,GOP04,DEM92,GOP92,DEM80,GOP80)
rownames(usa)<- data$Cod
```

```{r}
cor.usa <- cor(usa)
corrplot(cor.usa)
```
Realizamos algunas pruebas a los datos
```{r}
KMO(usa)
det(cor.usa)
bartlett.test(usa)
```
Vemos que el KMO es de un valor mediano. Por otro lado el determinante  de la matriz de correlación es prácticamente 0, lo que indíca multicolinealidad. Adicionalmente el teste de Barlett rechaza la hipótesis de que los datos vienen de una distribución por lo cuál el análisis factorial puede no ser aceptable

Como factor para elegir el número de factores considramos el número de componentes principales escogidas en los ejercicios anteriores
```{r}
factanal(usa, factors = 3, rotations = "none")

```
Como podemos ver, con tres factores podemos describir el 95% de la varianza.

## 7. Escalamiento Multidimensional: Encuentre una distancia entre los estados a partir de los datos. Utilizando escalamiento multidimensional, realizar un mapa de los estados + el Distrito de Columbia. ¿Qué agrupaciones podríıa realizar?

Comenzamos calculando la matriz de distancias entre variables, en este caso utilizaremos la distancia de _Manhattan_
```{r include=FALSE}
datis = read.csv("Data/Tidy/Elecciones_Parcial.csv")[,2:13]
head(datis)
attach

final <- datis %>%
  select(DEM12,GOP12,DEM04,GOP04,DEM92,GOP92,DEM80,GOP80)

rownames(final) = datis$Estado
head(final)

provMap <- scale(final) #Escalamos los datos para evitar problemas de dimensiones
provMap_d <- get_dist(provMap,method = "manhattan")#Calculamos la matriz de distancias

```
```{r echo=FALSE}
fviz_dist(provMap_d, lab_size = 8, gradient = list(low = "white",mid=NULL, high ="red"))
```  


Cómo podemo ver el Distrito de Columbia es el más alejado de los demás.

Ahora realizamos el escalamiénto multidimensional para dos dimensiones
```{r echo=FALSE}
provMap_r <- cmdscale(provMap_d,eig = TRUE,k = 2) #Calculamos dos dimensiones
provMap_r$GOF #Variabilidad explicada
pointsx <- provMap_r$points[,1]
pointsy <- provMap_r$points[,2]
columnforlabels = dimnames(provMap_r[[1]])[[1]]
plot(pointsx,pointsy, type = 'n',xlab = 'Dimension 1', ylab = 'Dimension 2', main = 'Mapa de Similitudes entre Paises')
columnforlabels = datis$Estado
colorforlabels = datis$REG
paleta = c("Magenta","Green","Red","Blue","Black")
text(pointsx,pointsy, labels = columnforlabels,cex = 0.8, col = paleta[colorforlabels])
legend('topright',legend = levels(colorforlabels),fill = paleta, title = 'Continente',cex = 0.4)

```

Como podemos ver, la agrupación conseguida puede verse similar a la determinada por regiones, a excepción del DC los estados de las mismas regiones tienden a estar más cerca entre sí, en especial los estados del _South_